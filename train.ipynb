{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 384)\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "# Load log records and crash sequences\n",
    "log_records = np.load('log_records.npy')\n",
    "print(log_records.shape)\n",
    "print(len(log_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crash_sequences = np.array([[0,0,1,1,1,0,0,0,0] for x in range(0, log_records.shape[0])])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_ratio = 0.8\n",
    "split_index = int(len(log_records) * train_ratio)\n",
    "\n",
    "train_records = log_records[:split_index]\n",
    "train_sequences = crash_sequences[:split_index]\n",
    "\n",
    "test_records = log_records[split_index:]\n",
    "test_sequences = crash_sequences[split_index:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCrashModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LogCrashModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define the dimensions for your model\n",
    "input_dim = 384\n",
    "hidden_dim = 128\n",
    "output_dim = 9  # Assuming you have two classes (crash or no crash)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LogCrashModel(input_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Step: 1/193, Loss: 3.2961719036102295\n",
      "Epoch: 1/10, Step: 25/193, Loss: 3.297083616256714\n",
      "Epoch: 1/10, Step: 49/193, Loss: 3.29618763923645\n",
      "Epoch: 1/10, Step: 73/193, Loss: 3.2961666584014893\n",
      "Epoch: 1/10, Step: 97/193, Loss: 3.2970762252807617\n",
      "Epoch: 1/10, Step: 121/193, Loss: 3.2970612049102783\n",
      "Epoch: 1/10, Step: 145/193, Loss: 3.296217679977417\n",
      "Epoch: 1/10, Step: 169/193, Loss: 3.296178102493286\n",
      "Epoch: 1/10, Step: 193/193, Loss: 3.296066999435425\n",
      "Epoch: 2/10, Step: 1/193, Loss: 3.2961838245391846\n",
      "Epoch: 2/10, Step: 25/193, Loss: 3.2962000370025635\n",
      "Epoch: 2/10, Step: 49/193, Loss: 3.296144485473633\n",
      "Epoch: 2/10, Step: 73/193, Loss: 3.296151876449585\n",
      "Epoch: 2/10, Step: 97/193, Loss: 3.2961950302124023\n",
      "Epoch: 2/10, Step: 121/193, Loss: 3.2970142364501953\n",
      "Epoch: 2/10, Step: 145/193, Loss: 3.296982765197754\n",
      "Epoch: 2/10, Step: 169/193, Loss: 3.2970120906829834\n",
      "Epoch: 2/10, Step: 193/193, Loss: 3.2962660789489746\n",
      "Epoch: 3/10, Step: 1/193, Loss: 3.2969751358032227\n",
      "Epoch: 3/10, Step: 25/193, Loss: 3.2969858646392822\n",
      "Epoch: 3/10, Step: 49/193, Loss: 3.2969963550567627\n",
      "Epoch: 3/10, Step: 73/193, Loss: 3.296154022216797\n",
      "Epoch: 3/10, Step: 97/193, Loss: 3.2961273193359375\n",
      "Epoch: 3/10, Step: 121/193, Loss: 3.2961509227752686\n",
      "Epoch: 3/10, Step: 145/193, Loss: 3.2961654663085938\n",
      "Epoch: 3/10, Step: 169/193, Loss: 3.2961556911468506\n",
      "Epoch: 3/10, Step: 193/193, Loss: 3.296243906021118\n",
      "Epoch: 4/10, Step: 1/193, Loss: 3.2961196899414062\n",
      "Epoch: 4/10, Step: 25/193, Loss: 3.296142578125\n",
      "Epoch: 4/10, Step: 49/193, Loss: 3.296117067337036\n",
      "Epoch: 4/10, Step: 73/193, Loss: 3.2961318492889404\n",
      "Epoch: 4/10, Step: 97/193, Loss: 3.2961463928222656\n",
      "Epoch: 4/10, Step: 121/193, Loss: 3.2976906299591064\n",
      "Epoch: 4/10, Step: 145/193, Loss: 3.296929121017456\n",
      "Epoch: 4/10, Step: 169/193, Loss: 3.296172857284546\n",
      "Epoch: 4/10, Step: 193/193, Loss: 3.29622483253479\n",
      "Epoch: 5/10, Step: 1/193, Loss: 3.296130895614624\n",
      "Epoch: 5/10, Step: 25/193, Loss: 3.296113967895508\n",
      "Epoch: 5/10, Step: 49/193, Loss: 3.2969024181365967\n",
      "Epoch: 5/10, Step: 73/193, Loss: 3.2961184978485107\n",
      "Epoch: 5/10, Step: 97/193, Loss: 3.2961323261260986\n",
      "Epoch: 5/10, Step: 121/193, Loss: 3.2968733310699463\n",
      "Epoch: 5/10, Step: 145/193, Loss: 3.296121597290039\n",
      "Epoch: 5/10, Step: 169/193, Loss: 3.2968637943267822\n",
      "Epoch: 5/10, Step: 193/193, Loss: 3.296205997467041\n",
      "Epoch: 6/10, Step: 1/193, Loss: 3.2975971698760986\n",
      "Epoch: 6/10, Step: 25/193, Loss: 3.2961008548736572\n",
      "Epoch: 6/10, Step: 49/193, Loss: 3.296113967895508\n",
      "Epoch: 6/10, Step: 73/193, Loss: 3.296879529953003\n",
      "Epoch: 6/10, Step: 97/193, Loss: 3.2960822582244873\n",
      "Epoch: 6/10, Step: 121/193, Loss: 3.2961018085479736\n",
      "Epoch: 6/10, Step: 145/193, Loss: 3.296107292175293\n",
      "Epoch: 6/10, Step: 169/193, Loss: 3.2961063385009766\n",
      "Epoch: 6/10, Step: 193/193, Loss: 3.296186685562134\n",
      "Epoch: 7/10, Step: 1/193, Loss: 3.297518730163574\n",
      "Epoch: 7/10, Step: 25/193, Loss: 3.296128511428833\n",
      "Epoch: 7/10, Step: 49/193, Loss: 3.2960939407348633\n",
      "Epoch: 7/10, Step: 73/193, Loss: 3.2967922687530518\n",
      "Epoch: 7/10, Step: 97/193, Loss: 3.2961044311523438\n",
      "Epoch: 7/10, Step: 121/193, Loss: 3.2960827350616455\n",
      "Epoch: 7/10, Step: 145/193, Loss: 3.296114683151245\n",
      "Epoch: 7/10, Step: 169/193, Loss: 3.2960541248321533\n",
      "Epoch: 7/10, Step: 193/193, Loss: 3.2961692810058594\n",
      "Epoch: 8/10, Step: 1/193, Loss: 3.2967662811279297\n",
      "Epoch: 8/10, Step: 25/193, Loss: 3.296064615249634\n",
      "Epoch: 8/10, Step: 49/193, Loss: 3.296764373779297\n",
      "Epoch: 8/10, Step: 73/193, Loss: 3.2960994243621826\n",
      "Epoch: 8/10, Step: 97/193, Loss: 3.2960855960845947\n",
      "Epoch: 8/10, Step: 121/193, Loss: 3.296738862991333\n",
      "Epoch: 8/10, Step: 145/193, Loss: 3.2961015701293945\n",
      "Epoch: 8/10, Step: 169/193, Loss: 3.2960758209228516\n",
      "Epoch: 8/10, Step: 193/193, Loss: 3.2961535453796387\n",
      "Epoch: 9/10, Step: 1/193, Loss: 3.296741247177124\n",
      "Epoch: 9/10, Step: 25/193, Loss: 3.296072244644165\n",
      "Epoch: 9/10, Step: 49/193, Loss: 3.296715021133423\n",
      "Epoch: 9/10, Step: 73/193, Loss: 3.296746253967285\n",
      "Epoch: 9/10, Step: 97/193, Loss: 3.296074628829956\n",
      "Epoch: 9/10, Step: 121/193, Loss: 3.2960729598999023\n",
      "Epoch: 9/10, Step: 145/193, Loss: 3.296071767807007\n",
      "Epoch: 9/10, Step: 169/193, Loss: 3.296048164367676\n",
      "Epoch: 9/10, Step: 193/193, Loss: 3.2960011959075928\n",
      "Epoch: 10/10, Step: 1/193, Loss: 3.296726942062378\n",
      "Epoch: 10/10, Step: 25/193, Loss: 3.296694755554199\n",
      "Epoch: 10/10, Step: 49/193, Loss: 3.2960662841796875\n",
      "Epoch: 10/10, Step: 73/193, Loss: 3.296065330505371\n",
      "Epoch: 10/10, Step: 97/193, Loss: 3.296671152114868\n",
      "Epoch: 10/10, Step: 121/193, Loss: 3.296035051345825\n",
      "Epoch: 10/10, Step: 145/193, Loss: 3.2960727214813232\n",
      "Epoch: 10/10, Step: 169/193, Loss: 3.296049118041992\n",
      "Epoch: 10/10, Step: 193/193, Loss: 3.295994758605957\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = log_records.shape[0]//num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = np.random.permutation(len(train_records)).astype(int)\n",
    "    train_records = train_records[permutation]\n",
    "    train_sequences = train_sequences[permutation]\n",
    "\n",
    "    for i in range(0, len(train_records), batch_size):\n",
    "        batch_records = torch.tensor(train_records[i:i+batch_size]).float()\n",
    "        batch_sequences = torch.tensor(train_sequences[i:i+batch_size]).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_records)\n",
    "        loss = criterion(outputs, batch_sequences)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\n",
    "                f'Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{len(train_records)}, Loss: {loss.item()}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2964)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinkbook\\AppData\\Local\\Temp\\ipykernel_63352\\624818299.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_records = torch.tensor(test_records).float()\n",
      "C:\\Users\\thinkbook\\AppData\\Local\\Temp\\ipykernel_63352\\624818299.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_sequences = torch.tensor(test_sequences).float()\n"
     ]
    }
   ],
   "source": [
    "test_records = torch.tensor(test_records).float()\n",
    "test_sequences = torch.tensor(test_sequences).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_records)\n",
    "    # crossentrophy loss\n",
    "    loss = criterion(outputs, test_sequences)\n",
    "    print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
